{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter11.SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "import os\n",
    "sc = SparkContext('local[*]', 'SparkML')\n",
    "parent_file = os.path.abspath('../')\n",
    "spam = sc.textFile('file://{}/files/spam.txt'.format(parent_file))\n",
    "normal = sc.textFile('file://{}/files/ham.txt'.format(parent_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseVector(10000, {0: 1.0, 365: 1.0, 455: 1.0, 509: 1.0, 1320: 1.0, 1363: 2.0, 1583: 1.0, 2321: 2.0, 2403: 1.0, 3289: 2.0, 3342: 1.0, 4995: 1.0, 5336: 1.0, 5706: 1.0, 5831: 1.0, 6052: 1.0, 6300: 1.0, 6582: 1.0, 6744: 1.0, 8971: 1.0, 8977: 1.0, 9232: 1.0, 9604: 1.0, 9646: 1.0, 9878: 1.0}), SparseVector(10000, {0: 1.0, 365: 1.0, 940: 1.0, 2220: 1.0, 3122: 1.0, 4460: 1.0, 4671: 1.0, 5336: 1.0, 5849: 1.0, 8479: 1.0, 9604: 1.0}), SparseVector(10000, {82: 1.0, 103: 1.0, 1091: 1.0, 1451: 1.0, 1819: 1.0, 2220: 1.0, 2321: 1.0, 2824: 1.0, 3317: 1.0, 3574: 1.0, 4373: 1.0, 4460: 1.0, 5763: 1.0, 6043: 1.0, 6052: 1.0, 6408: 1.0, 7119: 1.0, 7296: 1.0, 7656: 1.0, 9163: 1.0, 9399: 1.0, 9604: 1.0}), SparseVector(10000, {0: 2.0, 365: 1.0, 1683: 1.0, 2410: 1.0, 2634: 1.0, 4673: 1.0, 4837: 1.0, 5146: 1.0, 5172: 2.0, 5430: 1.0, 5880: 1.0, 6147: 1.0, 7094: 1.0, 7119: 1.0, 7566: 1.0, 7785: 1.0, 8242: 1.0, 9101: 1.0, 9241: 1.0, 9604: 1.0}), SparseVector(10000, {0: 1.0, 365: 2.0, 1395: 1.0, 1451: 1.0, 1458: 1.0, 1819: 1.0, 2701: 1.0, 3834: 1.0, 4323: 1.0, 4671: 1.0, 5336: 1.0, 5469: 1.0, 5878: 1.0, 6300: 1.0, 6384: 1.0, 6910: 1.0, 7296: 1.0, 9101: 2.0, 9604: 1.0})]\n",
      "[SparseVector(10000, {0: 1.0, 1162: 2.0, 2403: 1.0, 2809: 1.0, 3080: 1.0, 3317: 2.0, 4161: 1.0, 4770: 1.0, 5423: 1.0, 5651: 1.0, 5743: 1.0, 5831: 1.0, 6006: 1.0, 6827: 1.0, 6971: 1.0, 7069: 1.0, 7872: 1.0, 9150: 1.0, 9370: 1.0, 9521: 1.0, 9604: 1.0}), SparseVector(10000, {0: 1.0, 365: 1.0, 1320: 1.0, 1363: 1.0, 1627: 1.0, 2083: 1.0, 2321: 2.0, 2342: 1.0, 3317: 1.0, 3807: 1.0, 4164: 1.0, 5650: 1.0, 6300: 1.0, 6728: 1.0, 6827: 1.0, 7747: 1.0, 7770: 1.0, 8658: 1.0, 8898: 1.0, 9101: 2.0, 9367: 1.0, 9604: 1.0}), SparseVector(10000, {0: 1.0, 365: 2.0, 1162: 1.0, 1363: 1.0, 1649: 1.0, 2083: 1.0, 3317: 1.0, 3342: 1.0, 4839: 1.0, 5803: 1.0, 6260: 1.0, 6586: 1.0, 6637: 1.0, 6714: 1.0, 6727: 1.0, 7003: 1.0, 7872: 1.0, 7946: 1.0, 8322: 1.0, 9152: 1.0, 9571: 1.0, 9604: 1.0, 9978: 1.0}), SparseVector(10000, {0: 1.0, 365: 1.0, 997: 1.0, 1162: 1.0, 1363: 1.0, 1855: 2.0, 2090: 1.0, 2307: 1.0, 2403: 1.0, 3278: 1.0, 3870: 1.0, 4024: 1.0, 4164: 1.0, 4442: 1.0, 6043: 1.0, 6052: 1.0, 6093: 1.0, 6147: 1.0, 6299: 1.0, 6827: 1.0, 8242: 1.0, 9017: 1.0, 9604: 1.0}), SparseVector(10000, {0: 2.0, 365: 2.0, 777: 1.0, 1363: 2.0, 1649: 1.0, 2321: 1.0, 2527: 1.0, 5598: 1.0, 5642: 1.0, 6147: 1.0, 6618: 1.0, 6827: 2.0, 7349: 1.0, 7361: 1.0, 7872: 1.0, 8242: 1.0, 8385: 1.0, 8656: 1.0, 8909: 1.0, 9370: 1.0, 9604: 1.0, 9615: 1.0}), SparseVector(10000, {0: 2.0, 78: 1.0, 365: 1.0, 455: 1.0, 878: 1.0, 1363: 2.0, 1748: 1.0, 3140: 1.0, 4918: 1.0, 6147: 1.0, 6643: 1.0, 6971: 1.0, 7170: 1.0, 7296: 1.0, 7872: 1.0, 7946: 1.0, 9101: 1.0, 9138: 1.0, 9604: 1.0}), SparseVector(10000, {0: 1.0, 365: 1.0, 573: 1.0, 2321: 1.0, 2336: 1.0, 2963: 1.0, 4604: 1.0, 4992: 1.0, 6006: 1.0, 7069: 1.0, 7208: 1.0, 7271: 1.0, 7961: 1.0})]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个HashingTF实例来把文本映射为包含10000个特征的向量\n",
    "tf = HashingTF(numFeatures=10000)\n",
    "# 各邮件都被切分为单词，每个单词都被映射为一个特征\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(' ')))\n",
    "print(spamFeatures.collect())\n",
    "normalFeatures = normal.map(lambda email: tf.transform(email.split(' ')))\n",
    "print(normalFeatures.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[8] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建LabeledPoint数据集分别存放 垃圾邮件 和 正常邮件 的例子\n",
    "postiveExamlles = spamFeatures.map(lambda features: LabeledPoint(0, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "trainingData = postiveExamlles.union(negativeExamples)\n",
    "trainingData.cache()  # 因为逻辑回归是迭代算法，所以缓存训练数据RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SGD算法运行逻辑回归\n",
    "model = LogisticRegressionWithSGD.train(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for postive test example: 0\n",
      "Prediction for negative test example: 0\n"
     ]
    }
   ],
   "source": [
    "# 以 垃圾邮件 和 正常邮件 的例子分别进行测试\n",
    "# 首先使用一样的HashingTF特征来得到特征向量，然后对该向量应用得到的模型\n",
    "postTest = tf.transform('O M G GET cheap stuff by sending money to'.split(' '))\n",
    "negTest = tf.transform('Hi Dad, I started studying Spark the other'.split(' '))\n",
    "print('Prediction for postive test example: %g' % model.predict(postTest))\n",
    "print('Prediction for negative test example: %g' % model.predict(negTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.数据类型\n",
    "- 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 0.]\n",
      "[1.0,2.0,3.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "# 创建稠密向量 <1.0,2.0,3.0>\n",
    "denseVec1 = array([1.0, 2.0, 3.0, 0.0])  # Numpy 数组可以直接传给Mllib\n",
    "denseVec2 = Vectors.dense([1.0, 2.0, 3.0, 0.0])  # 或者使用Vectors类来创建\n",
    "print(denseVec1)\n",
    "print(denseVec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[0,1,3],[1.0,2.0,0.0])\n",
      "(4,[0,2,5],[1.0,2.0,0.0])\n"
     ]
    }
   ],
   "source": [
    "# 创建稀疏向量 该方法直接收向量的维度以及非0位的位置和对应的值\n",
    "# 这些数据可以用一个dict来传递，或者使用两个分别代表位置和值的list\n",
    "sparseVec1 = Vectors.sparse(4, {0: 1.0, 1: 2, 3: 0})\n",
    "sparseVec2 = Vectors.sparse(4, [0, 2, 5], [1.0, 2.0, 0.0])\n",
    "print(sparseVec1)\n",
    "print(sparseVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'hello hello world'\n",
    "rdd = sc.wholeTextFiles('file://{}/files/spam.txt'.format(parent_file))\n",
    "words = sentence.split()\n",
    "tf = HashingTF(10000)\n",
    "tfVectors = tf.transform(rdd).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(10000, {3861: 0.0, 7048: 0.0})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算IDF,然后计算TF-IDF向量\n",
    "idf = IDF()\n",
    "idfModel = idf.fit(tfVectors)\n",
    "tfIdfVectors = idfModel.transform(tfVectors)\n",
    "tfIdfVectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缩放\n",
    "控制平均值 和 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.7071, -0.7071, -0.7071]),\n",
       " DenseVector([0.7071, 0.7071, 0.7071])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScaler,Vectors,Normalizer,Word2Vec\n",
    "vectors = [Vectors.dense([1,2,3]),Vectors.dense([5,6,7])]\n",
    "dataset = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True,withStd=True)\n",
    "model = scaler.fit(dataset)\n",
    "result = model.transform(dataset)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler, Vectors, Normalizer, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.7071, -0.7071, -0.7071]),\n",
       " DenseVector([0.7071, 0.7071, 0.7071])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [Vectors.dense([1, 2, 3]), Vectors.dense([5, 6, 7])]\n",
    "dataset = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "model = scaler.fit(dataset)\n",
    "result = model.transform(dataset)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正规化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.2673, 0.5345, 0.8018]), DenseVector([0.4767, 0.5721, 0.6674])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalizer().transform(dataset).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"a b \" * 100 + \"a c \" * 10\n",
    "localDoc = [sentence, sentence]\n",
    "doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))\n",
    "model = Word2Vec().setVectorSize(10).setSeed(42).fit(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.267, -0.2691, 0.058, -0.0801, 0.1821, 0.4162, 0.0259, -0.2163, 0.1787, 0.0764])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model.transform(\"a\")\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类与回归\n",
    "- 线性回归\n",
    "- 逻辑回归\n",
    "- 支持向量机\n",
    "- 朴素贝叶斯\n",
    "- 决策树与随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
