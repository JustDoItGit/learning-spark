{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 键值对操作   \n",
    "### 1.动机\n",
    "- pairRDD: 提供并行操作各个键或跨节点重新进行数据分组的操作接口。\n",
    "- reduceByKey: 分别规约每个键对应的数据\n",
    "- join: 可以把两个RDD中键相同的的元素组合到一起，合并为一个RDD。\n",
    "\n",
    "### 2.创建PairRDD\n",
    "```\n",
    "rawRDD = sc.parallelize([\"打招呼 你好啊~~吃了吗??\",\"约 约吗美女?\"])\n",
    "pairRDD = rawRDD.map(lambda line : (line.split(' ')[0],line.split(' ')[1:]))\n",
    "pairRDD.collect()\n",
    "```\n",
    "### 3.PairRDD的转化操作\n",
    "#### 0.基本函数操作详见代码\n",
    "略\n",
    "#### 1.聚合操作   \n",
    "- 基础RDD聚合操作: reduce(),combine(),fold()\n",
    "- PairRDD聚合操作: reduceByKey().combineByKey(),foldByKey()\n",
    "- 数据分组: groupByKey().    groupByKey + mapValues 可以实现reduceByKey()同样功能，但前者效率低\n",
    "- 连接: join(),leftOuterJoin(),rightOuterJoin().\n",
    "- 数据排序: sortByKey()    \n",
    "### 4.PairRDD的行动操作\n",
    "- countByKey(): 对每个键嘴硬的元素分别计数\n",
    "- countByValue(): 按键对值进行统计  单词计数简化版\n",
    "- collectAsMap(): 结果以映射表的形式返回\n",
    "- lookup(key): 返回给定键对应的所有值 \n",
    "### 5.数据分区（进阶）\n",
    "使用自定义分区来提高效率，减少每次对不变的表进行混洗操作而消耗时间。例如使用rdd.partitionBy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext('local', 'PairRDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Quick': 1,\n",
       "             'Start': 2,\n",
       "             'Interactive': 2,\n",
       "             'Analysis': 2,\n",
       "             'with': 13,\n",
       "             'the': 41,\n",
       "             'Spark': 21,\n",
       "             'Shell': 2,\n",
       "             'Basics': 2,\n",
       "             'More': 2,\n",
       "             'on': 9,\n",
       "             'RDD': 9,\n",
       "             'Operations': 2,\n",
       "             'Caching': 2,\n",
       "             'Self-Contained': 2,\n",
       "             'Applications': 2,\n",
       "             'Where': 2,\n",
       "             'to': 28,\n",
       "             'Go': 2,\n",
       "             'from': 5,\n",
       "             'Here': 2,\n",
       "             'This': 5,\n",
       "             'tutorial': 1,\n",
       "             'provides': 2,\n",
       "             'a': 34,\n",
       "             'quick': 1,\n",
       "             'introduction': 1,\n",
       "             'using': 3,\n",
       "             'Spark.': 1,\n",
       "             'We': 5,\n",
       "             'will': 3,\n",
       "             'first': 4,\n",
       "             'introduce': 1,\n",
       "             'API': 1,\n",
       "             'through': 2,\n",
       "             'Spark’s': 3,\n",
       "             'interactive': 1,\n",
       "             'shell': 2,\n",
       "             '(in': 1,\n",
       "             'Python': 6,\n",
       "             'or': 7,\n",
       "             'Scala),': 1,\n",
       "             'then': 2,\n",
       "             'show': 1,\n",
       "             'how': 1,\n",
       "             'write': 2,\n",
       "             'applications': 3,\n",
       "             'in': 16,\n",
       "             'Java,': 3,\n",
       "             'Scala,': 1,\n",
       "             'and': 13,\n",
       "             'Python.': 3,\n",
       "             'See': 1,\n",
       "             'programming': 3,\n",
       "             'guide': 1,\n",
       "             'for': 4,\n",
       "             'more': 2,\n",
       "             'complete': 1,\n",
       "             'reference.': 1,\n",
       "             '': 71,\n",
       "             'To': 2,\n",
       "             'follow': 1,\n",
       "             'along': 1,\n",
       "             'this': 6,\n",
       "             'guide,': 2,\n",
       "             'download': 2,\n",
       "             'packaged': 1,\n",
       "             'release': 1,\n",
       "             'of': 13,\n",
       "             'website.': 1,\n",
       "             'Since': 1,\n",
       "             'we': 8,\n",
       "             'won’t': 1,\n",
       "             'be': 6,\n",
       "             'HDFS,': 1,\n",
       "             'you': 1,\n",
       "             'can': 12,\n",
       "             'package': 3,\n",
       "             'any': 2,\n",
       "             'version': 2,\n",
       "             'Hadoop.': 2,\n",
       "             'simple': 4,\n",
       "             'way': 2,\n",
       "             'learn': 1,\n",
       "             'API,': 3,\n",
       "             'as': 9,\n",
       "             'well': 1,\n",
       "             'powerful': 1,\n",
       "             'tool': 1,\n",
       "             'analyze': 1,\n",
       "             'data': 5,\n",
       "             'interactively.': 1,\n",
       "             'It': 2,\n",
       "             'is': 11,\n",
       "             'available': 1,\n",
       "             'either': 1,\n",
       "             'Scala': 8,\n",
       "             '(which': 1,\n",
       "             'runs': 1,\n",
       "             'Java': 4,\n",
       "             'VM': 1,\n",
       "             'thus': 1,\n",
       "             'good': 1,\n",
       "             'use': 11,\n",
       "             'existing': 1,\n",
       "             'libraries)': 1,\n",
       "             'it': 1,\n",
       "             'by': 4,\n",
       "             'running': 4,\n",
       "             'following': 1,\n",
       "             'directory:': 2,\n",
       "             './bin/spark-shell': 1,\n",
       "             'primary': 1,\n",
       "             'abstraction': 1,\n",
       "             'distributed': 1,\n",
       "             'collection': 1,\n",
       "             'items': 3,\n",
       "             'called': 2,\n",
       "             'Resilient': 1,\n",
       "             'Distributed': 1,\n",
       "             'Dataset': 1,\n",
       "             '(RDD).': 1,\n",
       "             'RDDs': 2,\n",
       "             'created': 1,\n",
       "             'Hadoop': 1,\n",
       "             'InputFormats': 1,\n",
       "             '(such': 1,\n",
       "             'HDFS': 1,\n",
       "             'files)': 1,\n",
       "             'transforming': 1,\n",
       "             'other': 2,\n",
       "             'RDDs.': 2,\n",
       "             'Let’s': 3,\n",
       "             'make': 2,\n",
       "             'new': 6,\n",
       "             'text': 2,\n",
       "             'README': 1,\n",
       "             'file': 4,\n",
       "             'source': 1,\n",
       "             'scala>': 13,\n",
       "             'val': 9,\n",
       "             'textFile': 1,\n",
       "             '=': 21,\n",
       "             'sc.textFile(\"README.md\")': 1,\n",
       "             'textFile:': 1,\n",
       "             'spark.RDD[String]': 3,\n",
       "             'spark.MappedRDD@2ee9b6e3': 1,\n",
       "             'have': 1,\n",
       "             'actions,': 1,\n",
       "             'which': 5,\n",
       "             'return': 3,\n",
       "             'values,': 1,\n",
       "             'transformations,': 1,\n",
       "             'pointers': 1,\n",
       "             'start': 2,\n",
       "             'few': 1,\n",
       "             'actions:': 2,\n",
       "             'textFile.count()': 1,\n",
       "             '//': 4,\n",
       "             'Number': 1,\n",
       "             'res0:': 1,\n",
       "             'Long': 5,\n",
       "             '126': 1,\n",
       "             'textFile.first()': 1,\n",
       "             'First': 1,\n",
       "             'item': 1,\n",
       "             'res1:': 1,\n",
       "             'String': 1,\n",
       "             '#': 7,\n",
       "             'Apache': 1,\n",
       "             'Now': 1,\n",
       "             'let’s': 2,\n",
       "             'transformation.': 1,\n",
       "             'filter': 1,\n",
       "             'transformation': 1,\n",
       "             'subset': 1,\n",
       "             'file.': 2,\n",
       "             'linesWithSpark': 2,\n",
       "             'textFile.filter(line': 2,\n",
       "             '=>': 11,\n",
       "             'line.contains(\"Spark\"))': 1,\n",
       "             'linesWithSpark:': 1,\n",
       "             'spark.FilteredRDD@7dd4af09': 1,\n",
       "             'chain': 1,\n",
       "             'together': 1,\n",
       "             'transformations': 3,\n",
       "             'line.contains(\"Spark\")).count()': 1,\n",
       "             'How': 1,\n",
       "             'many': 1,\n",
       "             'lines': 2,\n",
       "             'contain': 1,\n",
       "             '\"Spark\"?': 1,\n",
       "             'res3:': 1,\n",
       "             '15': 3,\n",
       "             'actions': 1,\n",
       "             'used': 2,\n",
       "             'complex': 1,\n",
       "             'computations.': 1,\n",
       "             'say': 1,\n",
       "             'want': 1,\n",
       "             'find': 3,\n",
       "             'line': 3,\n",
       "             'most': 1,\n",
       "             'words:': 1,\n",
       "             'textFile.map(line': 2,\n",
       "             'line.split(\"': 3,\n",
       "             '\").size).reduce((a,': 2,\n",
       "             'b)': 6,\n",
       "             'if': 1,\n",
       "             '(a': 1,\n",
       "             '>': 1,\n",
       "             'else': 1,\n",
       "             'res4:': 1,\n",
       "             'maps': 1,\n",
       "             'an': 5,\n",
       "             'integer': 1,\n",
       "             'value,': 1,\n",
       "             'creating': 1,\n",
       "             'RDD.': 1,\n",
       "             'reduce': 2,\n",
       "             'that': 8,\n",
       "             'largest': 1,\n",
       "             'count.': 1,\n",
       "             'The': 2,\n",
       "             'arguments': 1,\n",
       "             'map': 1,\n",
       "             'are': 2,\n",
       "             'function': 2,\n",
       "             'literals': 1,\n",
       "             '(closures),': 1,\n",
       "             'language': 1,\n",
       "             'feature': 1,\n",
       "             'Scala/Java': 1,\n",
       "             'library.': 1,\n",
       "             'For': 7,\n",
       "             'example,': 2,\n",
       "             'easily': 1,\n",
       "             'call': 1,\n",
       "             'functions': 2,\n",
       "             'declared': 1,\n",
       "             'elsewhere.': 1,\n",
       "             'We’ll': 2,\n",
       "             'Math.max()': 1,\n",
       "             'code': 1,\n",
       "             'easier': 1,\n",
       "             'understand:': 1,\n",
       "             'import': 5,\n",
       "             'java.lang.Math': 2,\n",
       "             'Math.max(a,': 1,\n",
       "             'b))': 1,\n",
       "             'res5:': 1,\n",
       "             'Int': 1,\n",
       "             'One': 1,\n",
       "             'common': 1,\n",
       "             'flow': 1,\n",
       "             'pattern': 1,\n",
       "             'MapReduce,': 1,\n",
       "             'popularized': 1,\n",
       "             'implement': 1,\n",
       "             'MapReduce': 1,\n",
       "             'flows': 1,\n",
       "             'easily:': 1,\n",
       "             'wordCounts': 1,\n",
       "             'textFile.flatMap(line': 1,\n",
       "             '\")).map(word': 1,\n",
       "             '(word,': 1,\n",
       "             '1)).reduceByKey((a,': 1,\n",
       "             '+': 1,\n",
       "             'wordCounts:': 1,\n",
       "             'spark.RDD[(String,': 1,\n",
       "             'Int)]': 2,\n",
       "             'spark.ShuffledAggregatedRDD@71f027b8': 1,\n",
       "             'Here,': 1,\n",
       "             'combined': 1,\n",
       "             'flatMap,': 1,\n",
       "             'map,': 1,\n",
       "             'reduceByKey': 1,\n",
       "             'compute': 1,\n",
       "             'per-word': 1,\n",
       "             'counts': 3,\n",
       "             '(String,': 1,\n",
       "             'Int)': 1,\n",
       "             'pairs.': 1,\n",
       "             'collect': 2,\n",
       "             'word': 1,\n",
       "             'our': 4,\n",
       "             'shell,': 2,\n",
       "             'action:': 1,\n",
       "             'wordCounts.collect()': 1,\n",
       "             'res6:': 1,\n",
       "             'Array[(String,': 1,\n",
       "             'Array((means,1),': 1,\n",
       "             '(under,2),': 1,\n",
       "             '(this,3),': 1,\n",
       "             '(Because,1),': 1,\n",
       "             '(Python,2),': 1,\n",
       "             '(agree,1),': 1,\n",
       "             '(cluster.,1),': 1,\n",
       "             '...)': 1,\n",
       "             'also': 4,\n",
       "             'supports': 1,\n",
       "             'pulling': 1,\n",
       "             'sets': 1,\n",
       "             'into': 1,\n",
       "             'cluster-wide': 1,\n",
       "             'in-memory': 1,\n",
       "             'cache.': 1,\n",
       "             'very': 3,\n",
       "             'useful': 1,\n",
       "             'when': 4,\n",
       "             'accessed': 1,\n",
       "             'repeatedly,': 1,\n",
       "             'such': 1,\n",
       "             'querying': 1,\n",
       "             'small': 1,\n",
       "             '“hot”': 1,\n",
       "             'dataset': 2,\n",
       "             'iterative': 1,\n",
       "             'algorithm': 1,\n",
       "             'like': 2,\n",
       "             'PageRank.': 1,\n",
       "             'As': 1,\n",
       "             'mark': 1,\n",
       "             'cached:': 1,\n",
       "             'linesWithSpark.cache()': 1,\n",
       "             'res7:': 1,\n",
       "             'spark.FilteredRDD@17e51082': 1,\n",
       "             'linesWithSpark.count()': 2,\n",
       "             'res8:': 1,\n",
       "             '19': 2,\n",
       "             'res9:': 1,\n",
       "             'may': 2,\n",
       "             'seem': 1,\n",
       "             'silly': 1,\n",
       "             'explore': 1,\n",
       "             'cache': 1,\n",
       "             '100-line': 1,\n",
       "             'interesting': 1,\n",
       "             'part': 2,\n",
       "             'these': 1,\n",
       "             'same': 1,\n",
       "             'large': 1,\n",
       "             'sets,': 1,\n",
       "             'even': 1,\n",
       "             'they': 1,\n",
       "             'striped': 1,\n",
       "             'across': 1,\n",
       "             'tens': 1,\n",
       "             'hundreds': 1,\n",
       "             'nodes.': 1,\n",
       "             'You': 2,\n",
       "             'do': 1,\n",
       "             'interactively': 1,\n",
       "             'connecting': 1,\n",
       "             'bin/spark-shell': 1,\n",
       "             'cluster,': 2,\n",
       "             'described': 1,\n",
       "             'guide.': 1,\n",
       "             'Suppose': 1,\n",
       "             'wish': 1,\n",
       "             'self-contained': 1,\n",
       "             'application': 6,\n",
       "             'API.': 1,\n",
       "             'walk': 1,\n",
       "             '(with': 2,\n",
       "             'sbt),': 1,\n",
       "             'Maven),': 1,\n",
       "             'create': 2,\n",
       "             'Scala–so': 1,\n",
       "             'simple,': 1,\n",
       "             'fact,': 1,\n",
       "             'it’s': 1,\n",
       "             'named': 1,\n",
       "             'SimpleApp.scala:': 1,\n",
       "             '/*': 1,\n",
       "             'SimpleApp.scala': 2,\n",
       "             '*/': 1,\n",
       "             'org.apache.spark.SparkContext': 1,\n",
       "             'org.apache.spark.SparkContext._': 1,\n",
       "             'org.apache.spark.SparkConf': 1,\n",
       "             'object': 2,\n",
       "             'SimpleApp': 1,\n",
       "             '{': 2,\n",
       "             'def': 1,\n",
       "             'main(args:': 1,\n",
       "             'Array[String])': 1,\n",
       "             'logFile': 1,\n",
       "             '\"YOUR_SPARK_HOME/README.md\"': 1,\n",
       "             'Should': 1,\n",
       "             'some': 1,\n",
       "             'your': 4,\n",
       "             'system': 1,\n",
       "             'conf': 1,\n",
       "             'SparkConf().setAppName(\"Simple': 1,\n",
       "             'Application\")': 1,\n",
       "             'sc': 1,\n",
       "             'SparkContext(conf)': 1,\n",
       "             'logData': 1,\n",
       "             'sc.textFile(logFile,': 1,\n",
       "             '2).cache()': 1,\n",
       "             'numAs': 1,\n",
       "             'logData.filter(line': 2,\n",
       "             'line.contains(\"a\")).count()': 1,\n",
       "             'numBs': 1,\n",
       "             'line.contains(\"b\")).count()': 1,\n",
       "             'println(\"Lines': 1,\n",
       "             'a:': 2,\n",
       "             '%s,': 1,\n",
       "             'Lines': 3,\n",
       "             'b:': 2,\n",
       "             '%s\".format(numAs,': 1,\n",
       "             'numBs))': 1,\n",
       "             '}': 2,\n",
       "             'Note': 2,\n",
       "             'should': 2,\n",
       "             'define': 1,\n",
       "             'main()': 1,\n",
       "             'method': 1,\n",
       "             'instead': 1,\n",
       "             'extending': 1,\n",
       "             'scala.App.': 1,\n",
       "             'Subclasses': 1,\n",
       "             'scala.App': 1,\n",
       "             'not': 1,\n",
       "             'work': 2,\n",
       "             'correctly.': 1,\n",
       "             'program': 1,\n",
       "             'just': 1,\n",
       "             'number': 2,\n",
       "             'containing': 4,\n",
       "             '‘a’': 1,\n",
       "             '‘b’': 1,\n",
       "             'README.': 1,\n",
       "             'you’ll': 1,\n",
       "             'need': 2,\n",
       "             'replace': 1,\n",
       "             'YOUR_SPARK_HOME': 1,\n",
       "             'location': 1,\n",
       "             'where': 1,\n",
       "             'installed.': 1,\n",
       "             'Unlike': 1,\n",
       "             'earlier': 1,\n",
       "             'examples': 2,\n",
       "             'initializes': 1,\n",
       "             'its': 1,\n",
       "             'own': 1,\n",
       "             'SparkContext,': 1,\n",
       "             'initialize': 1,\n",
       "             'SparkContext': 2,\n",
       "             'program.': 2,\n",
       "             'pass': 1,\n",
       "             'constructor': 1,\n",
       "             'SparkConf': 1,\n",
       "             'contains': 1,\n",
       "             'information': 1,\n",
       "             'about': 1,\n",
       "             'application.': 1,\n",
       "             'Our': 1,\n",
       "             'depends': 2,\n",
       "             'so': 1,\n",
       "             'we’ll': 2,\n",
       "             'include': 1,\n",
       "             'sbt': 3,\n",
       "             'configuration': 1,\n",
       "             'file,': 1,\n",
       "             'simple.sbt,': 1,\n",
       "             'explains': 1,\n",
       "             'dependency.': 1,\n",
       "             'adds': 1,\n",
       "             'repository': 1,\n",
       "             'on:': 1,\n",
       "             'name': 1,\n",
       "             ':=': 3,\n",
       "             '\"Simple': 1,\n",
       "             'Project\"': 1,\n",
       "             '\"1.0\"': 1,\n",
       "             'scalaVersion': 1,\n",
       "             '\"2.10.5\"': 1,\n",
       "             'libraryDependencies': 1,\n",
       "             '+=': 1,\n",
       "             '\"org.apache.spark\"': 1,\n",
       "             '%%': 1,\n",
       "             '\"spark-core\"': 1,\n",
       "             '%': 1,\n",
       "             '\"1.6.2\"': 1,\n",
       "             'correctly,': 1,\n",
       "             'layout': 2,\n",
       "             'simple.sbt': 1,\n",
       "             'according': 1,\n",
       "             'typical': 1,\n",
       "             'directory': 3,\n",
       "             'structure.': 1,\n",
       "             'Once': 1,\n",
       "             'place,': 1,\n",
       "             'JAR': 1,\n",
       "             'application’s': 1,\n",
       "             'code,': 1,\n",
       "             'spark-submit': 4,\n",
       "             'script': 1,\n",
       "             'run': 3,\n",
       "             'Your': 1,\n",
       "             'look': 1,\n",
       "             '$': 3,\n",
       "             '.': 2,\n",
       "             './simple.sbt': 1,\n",
       "             './src': 1,\n",
       "             './src/main': 1,\n",
       "             './src/main/scala': 1,\n",
       "             './src/main/scala/SimpleApp.scala': 1,\n",
       "             'Package': 1,\n",
       "             'jar': 1,\n",
       "             '...': 2,\n",
       "             '[info]': 1,\n",
       "             'Packaging': 1,\n",
       "             '{..}/{..}/target/scala-2.10/simple-project_2.10-1.0.jar': 1,\n",
       "             'Use': 1,\n",
       "             'YOUR_SPARK_HOME/bin/spark-submit': 1,\n",
       "             '\\\\': 3,\n",
       "             '--class': 1,\n",
       "             '\"SimpleApp\"': 1,\n",
       "             '--master': 1,\n",
       "             'local[4]': 1,\n",
       "             'target/scala-2.10/simple-project_2.10-1.0.jar': 1,\n",
       "             '46,': 1,\n",
       "             '23': 1,\n",
       "             'Congratulations': 1,\n",
       "             'application!': 1,\n",
       "             'in-depth': 1,\n",
       "             'overview': 1,\n",
       "             'see': 1,\n",
       "             '“Programming': 1,\n",
       "             'Guides”': 1,\n",
       "             'menu': 1,\n",
       "             'components.': 1,\n",
       "             'head': 1,\n",
       "             'deployment': 1,\n",
       "             'overview.': 1,\n",
       "             'Finally,': 1,\n",
       "             'includes': 1,\n",
       "             'several': 1,\n",
       "             'samples': 1,\n",
       "             '(Scala,': 1,\n",
       "             'Python,': 1,\n",
       "             'R).': 1,\n",
       "             'them': 1,\n",
       "             'follows:': 1,\n",
       "             'run-example:': 1,\n",
       "             './bin/run-example': 1,\n",
       "             'SparkPi': 1,\n",
       "             'examples,': 2,\n",
       "             'directly:': 2,\n",
       "             './bin/spark-submit': 2,\n",
       "             'examples/src/main/python/pi.py': 1,\n",
       "             'R': 1,\n",
       "             'examples/src/main/r/dataframe.R': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordCount\n",
    "import os\n",
    "words = sc.textFile('file://' + os.path.abspath('.') +\n",
    "                    '/quickstart.txt').flatMap(lambda line: line.split(' '))\n",
    "\n",
    "# wordCount = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "# wordCount.collect()\n",
    "wordCount = words.countByValue()\n",
    "wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('打招呼', '你好啊~~吃了吗??'), ('约', '约吗美女?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRDD = sc.parallelize([\"打招呼 你好啊~~吃了吗??\", \"约 约吗美女?\"])\n",
    "pairRDD = rawRDD.map(lambda line: (line.split(' ')[0], line.split(' ')[1]))\n",
    "pairRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'打': 1, '你': 1, '约': 2})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRDD = sc.parallelize([\"打招呼 你好啊~~吃了吗??\", \"约 约吗美女?\"])\n",
    "pairRDD = rawRDD.flatMap(lambda line: line.split(' '))\n",
    "pairRDD.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairRDD转化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD = sc.parallelize([[1, 2], [3, 4], [3, 6], [4, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10), (4, 6)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.reduceByKey 合并相同Key的值\n",
    "pair1 = pairRDD.reduceByKey(lambda x, y: x + y)\n",
    "pair1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x122e68908>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x122e68748>),\n",
       " (4, <pyspark.resultiterable.ResultIterable at 0x122e685f8>)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.groupByKey  对相同Key的值分组\n",
    "pair2 = pairRDD.groupByKey()\n",
    "pair2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 7), (3, 9), (3, 11), (4, 11)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.mapValues   对RDD中的每个值应用一个函数而不改变键\n",
    "pair3 = pairRDD.mapValues(lambda x: x + 5)\n",
    "pair3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0),\n",
       " (1, 1),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 5)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.flatMapValues 对RDD中的每个值应用一个返回迭代器的函数，对于每个元素都生成一个对应原键的键值对记录。\n",
    "pair4 = pairRDD.flatMapValues(lambda x: range(x))\n",
    "# pair4 = pairRDD.flatMapValues(lambda x: (range(x)))\n",
    "pair4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3, 4]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.keys 返回Key的RDD\n",
    "keysRDD = pairRDD.keys()\n",
    "keysRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.values 返回value的RDD\n",
    "valuesRDD = pairRDD.values()\n",
    "valuesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [3, 6], [4, 6]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.sortByKey 按键排序\n",
    "sortedRDD = pairRDD.sortByKey()\n",
    "sortedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 针对两个RDD的转换操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对两个RDD的转换操作\n",
    "rdd1 = sc.parallelize([[1, 2], [3, 4], [3, 6]])\n",
    "rdd2 = sc.parallelize([[3, 9], [4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.substractByKey  删掉key值重复的元素\n",
    "subRDD = rdd1.subtractByKey(rdd2)\n",
    "subRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.join  内连接\n",
    "joinRDD = rdd1.join(rdd2)\n",
    "joinRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (9, 4)), (3, (9, 6))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinRDD = rdd2.join(rdd1)\n",
    "joinRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (None, 5)), (3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.rightOuterJoin 确保第2个RDD的键必须存在   右外连接\n",
    "rightOuterRDD = rdd1.rightOuterJoin(rdd2)\n",
    "rightOuterRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 2)), (3, (4, 4)), (3, (4, 6)), (3, (6, 4)), (3, (6, 6))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.leftOuterJoin 确保第1个RDD的键必须存在  左外连接\n",
    "leftOuterRDD = rdd1.leftOuterJoin(rdd1)\n",
    "leftOuterRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1224dceb8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1224dc6d8>)),\n",
       " (1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1224dcf98>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1224dc358>)),\n",
       " (3,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1224dce48>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1224dcac8>))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.cogroup 将两个RDD中拥有相同键的数据分组到一起\n",
    "coRDD = rdd1.cogroup(rdd2)\n",
    "coRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对pairRDD的value进行筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Interactive', 'Interactive Analysis with the Spark Shell'),\n",
       " ('Self-Contained', 'Self-Contained Applications'),\n",
       " ('Interactive', 'Interactive Analysis with the Spark Shell'),\n",
       " ('./bin/spark-shell', './bin/spark-shell'),\n",
       " ('textFile:', 'textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3'),\n",
       " ('linesWithSpark:',\n",
       "  'linesWithSpark: spark.RDD[String] = spark.FilteredRDD@7dd4af09'),\n",
       " ('wordCounts:',\n",
       "  'wordCounts: spark.RDD[(String, Int)] = spark.ShuffledAggregatedRDD@71f027b8'),\n",
       " ('Self-Contained', 'Self-Contained Applications'),\n",
       " ('scalaVersion', 'scalaVersion := \"2.10.5\"'),\n",
       " ('libraryDependencies',\n",
       "  'libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.6.2\"'),\n",
       " ('./simple.sbt', './simple.sbt'),\n",
       " ('./src/main', './src/main'),\n",
       " ('./src/main/scala', './src/main/scala'),\n",
       " ('./src/main/scala/SimpleApp.scala', './src/main/scala/SimpleApp.scala'),\n",
       " ('Congratulations',\n",
       "  'Congratulations on running your first Spark application!'),\n",
       " ('Finally,',\n",
       "  'Finally, Spark includes several samples in the examples directory (Scala, Java, Python, R). You can run them as follows:'),\n",
       " ('./bin/run-example', './bin/run-example SparkPi'),\n",
       " ('./bin/spark-submit', './bin/spark-submit examples/src/main/python/pi.py'),\n",
       " ('./bin/spark-submit', './bin/spark-submit examples/src/main/r/dataframe.R')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('file://{}/quickstart.txt'.format(os.path.abspath('.')))\n",
    "pairsRDD = lines.map(lambda line: (line.split(' ')[0], line))\n",
    "limitLengthRDD = pairsRDD.filter(lambda keyValue: len(keyValue[0]) > 7)\n",
    "limitLengthRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚合操作  按key值计算平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1.5), ('b', 1.0), ('c', 6.5)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = sc.parallelize(\n",
    "    [['a', 1], ['b', 2], ['c', 3], ['a', 2], ['b', 0], ['c', 10]])\n",
    "aveRDD = pairRDD.mapValues(lambda x: (x, 1)).reduceByKey(\n",
    "    lambda x, y: (x[0] + y[0], x[1] + y[1])).mapValues(lambda x: x[0]/x[1])\n",
    "aveRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (3, 2)), ('b', (2, 2)), ('c', (13, 2))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 1.5), ('b', 1.0), ('c', 6.5)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumRDD = pairRDD.mapValues(lambda x: (x, 1)).reduceByKey(\n",
    "    lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print(sumRDD.collect())\n",
    "aveRDD = sumRDD.mapValues(lambda x: x[0]/x[1])\n",
    "aveRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount = pairRDD.combineByKey((lambda x: (x, 1)),\n",
    "                                (lambda x, y: (x[0]+y, x[1]+1)),\n",
    "                                (lambda x, y: (x[0]+y[0], x[1]+y[1])))\n",
    "sumCount.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1.5, 'b': 1.0, 'c': 6.5}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aveRDD = sumCount.mapValues(lambda x: x[0]/x[1])\n",
    "aveRDD.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (3, 2)), ('b', (2, 2)), ('c', (13, 2))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': (3, 2), 'b': (2, 2), 'c': (13, 2)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 86.0 failed 1 times, most recent failure: Lost task 0.0 in stage 86.0 (TID 83, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'xy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'xy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-1cf85a401c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m sumCount = pairRDD.combineByKey((lambda x: (x, 1)), (lambda x, y: (\n\u001b[1;32m      2\u001b[0m     x[0] + y, x[1] + 1)), (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msumCount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \"\"\"\n\u001b[0;32m-> 1587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 86.0 failed 1 times, most recent failure: Lost task 0.0 in stage 86.0 (TID 83, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'xy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/cool/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'xy'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# 现在这种写法错误\n",
    "sumCount = pairRDD.combineByKey((lambda x: (x, 1)),\n",
    "                                (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                                (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "sumCount.map(lambda key, xy: (key, xy[0]/xy[1])).collectAsMap()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
